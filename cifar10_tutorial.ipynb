{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "from center_loss import center_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:(50000, 32, 32, 3), y_train:(50000,)\n",
      "x_test:(10000, 32, 32, 3), y_test:(10000,)\n",
      "y_train_ohe: (50000, 10)\n",
      "y_test_ohe: (10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "ohe = OneHotEncoder()\n",
    "y_train_ohe = ohe.fit_transform(y_train.reshape(-1,1)).toarray().astype('float32')\n",
    "y_test_ohe = ohe.transform(y_test.reshape(-1,1)).toarray().astype('float32')\n",
    "print('x_train:{}, y_train:{}'.format(x_train.shape, y_train.shape))\n",
    "print('x_test:{}, y_test:{}'.format(x_test.shape, y_test.shape))\n",
    "print('y_train_ohe:', y_train_ohe.shape)\n",
    "print('y_test_ohe:', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['seed'] = 0\n",
    "params['embedding'] = 16\n",
    "params['n_classes'] = 10\n",
    "params['labels'] = np.unique(y_train).astype('int')\n",
    "params['batch_size'] = 128\n",
    "params['lambda'] = 0.003\n",
    "params['alpha'] = 0.1\n",
    "params['feed_limit'] = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model = LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 16:47:11.782063 139932902004480 deprecation.py:323] From <ipython-input-4-5c2c5e769059>:9: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0830 16:47:11.787720 139932902004480 deprecation.py:506] From /home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0830 16:47:12.011117 139932902004480 deprecation.py:323] From <ipython-input-4-5c2c5e769059>:11: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "W0830 16:47:12.158684 139932902004480 deprecation.py:323] From <ipython-input-4-5c2c5e769059>:17: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0830 16:47:12.379362 139932902004480 deprecation.py:323] From <ipython-input-4-5c2c5e769059>:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(params['seed'])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, [None,])\n",
    "y_ohe = tf.placeholder(tf.float32, [None, params['n_classes']])\n",
    "centroids = tf.placeholder(tf.float32, [params['n_classes'], params['embedding']])\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=x, filters=6, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv1')\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "conv1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=conv1, filters=16, kernel_size=(5,5), strides=(1,1), padding='valid', name='conv2')\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "conv2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2,2), strides=(2,2), padding='valid')\n",
    "\n",
    "flatten = tf.layers.flatten(conv2)\n",
    "\n",
    "fc1 = tf.layers.dense(flatten, units=120, use_bias=True, activation=tf.nn.relu, name='fc1')\n",
    "fc2 = tf.layers.dense(fc1, units=84, use_bias=True, activation=tf.nn.relu, name='fc2')\n",
    "embedding_layer = tf.layers.dense(fc2, units=params['embedding'], use_bias=True, activation=None)\n",
    "\n",
    "clf_layer = tf.layers.dense(embedding_layer, units=params['n_classes'], activation=None, use_bias=False, name='clf_layer')\n",
    "softmax_layer = tf.nn.softmax(clf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1:(?, 14, 14, 6)\n",
      "conv2:(?, 5, 5, 16)\n",
      "fc1:(?, 120)\n",
      "fc2:(?, 84)\n",
      "embedding_layer:(?, 16)\n",
      "clf_layer:(?, 10)\n"
     ]
    }
   ],
   "source": [
    "print('conv1:{}'.format(conv1.shape))\n",
    "print('conv2:{}'.format(conv2.shape))\n",
    "print('fc1:{}'.format(fc1.shape))\n",
    "print('fc2:{}'.format(fc2.shape))\n",
    "print('embedding_layer:{}'.format(embedding_layer.shape))\n",
    "print('clf_layer:{}'.format(clf_layer.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0830 16:47:12.873792 139932902004480 deprecation.py:323] From /home/kim1/anaconda3/envs/rok/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "eta = 1e-3\n",
    "epsilon = 1e-5\n",
    "softmax_loss = tf.reduce_sum(tf.maximum(tf.multiply(-tf.log(softmax_layer + epsilon), y_ohe), 0))\n",
    "loss_add = center_loss(embedding_layer=embedding_layer, centroids=centroids, y=y, params=params)\n",
    "loss = softmax_loss + loss_add\n",
    "train_model = tf.train.GradientDescentOptimizer(learning_rate=eta).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1    27.92sec \n",
      "           train(loss:18411.3555, accuracy0.3591) \n",
      "           test (loss:18406.7539, accuracy0.3602)\n",
      "\n",
      "epoch:2    28.75sec \n",
      "           train(loss:15630.8184, accuracy0.4516) \n",
      "           test (loss:15783.8838, accuracy0.4469)\n",
      "\n",
      "epoch:3    27.13sec \n",
      "           train(loss:14593.6074, accuracy0.4876) \n",
      "           test (loss:14821.1787, accuracy0.4770)\n",
      "\n",
      "epoch:4    28.23sec \n",
      "           train(loss:13614.8281, accuracy0.5208) \n",
      "           test (loss:14115.0391, accuracy0.5001)\n",
      "\n",
      "epoch:5    27.30sec \n",
      "           train(loss:12961.9180, accuracy0.5444) \n",
      "           test (loss:13578.9365, accuracy0.5166)\n",
      "\n",
      "epoch:6    28.17sec \n",
      "           train(loss:12214.1387, accuracy0.5720) \n",
      "           test (loss:13254.2852, accuracy0.5355)\n",
      "\n",
      "epoch:7    26.92sec \n",
      "           train(loss:11577.2344, accuracy0.5969) \n",
      "           test (loss:12755.5166, accuracy0.5538)\n",
      "\n",
      "epoch:8    28.08sec \n",
      "           train(loss:11591.2451, accuracy0.5918) \n",
      "           test (loss:12890.7031, accuracy0.5402)\n",
      "\n",
      "epoch:9    27.01sec \n",
      "           train(loss:11388.6885, accuracy0.6011) \n",
      "           test (loss:12977.4570, accuracy0.5452)\n",
      "\n",
      "epoch:10    28.04sec \n",
      "           train(loss:10688.1055, accuracy0.6249) \n",
      "           test (loss:12589.9053, accuracy0.5614)\n",
      "\n",
      "epoch:11    27.16sec \n",
      "           train(loss:10072.7080, accuracy0.6496) \n",
      "           test (loss:12328.3369, accuracy0.5670)\n",
      "\n",
      "epoch:12    28.76sec \n",
      "           train(loss:9892.1943, accuracy0.6558) \n",
      "           test (loss:12388.2930, accuracy0.5723)\n",
      "\n",
      "epoch:13    27.30sec \n",
      "           train(loss:9718.9824, accuracy0.6576) \n",
      "           test (loss:12406.9873, accuracy0.5635)\n",
      "\n",
      "epoch:14    28.20sec \n",
      "           train(loss:9061.5488, accuracy0.6872) \n",
      "           test (loss:12224.3477, accuracy0.5806)\n",
      "\n",
      "epoch:15    27.23sec \n",
      "           train(loss:9082.5898, accuracy0.6852) \n",
      "           test (loss:12424.8984, accuracy0.5786)\n",
      "\n",
      "epoch:16    28.10sec \n",
      "           train(loss:8839.1182, accuracy0.6968) \n",
      "           test (loss:12209.0586, accuracy0.5804)\n",
      "\n",
      "epoch:17    27.36sec \n",
      "           train(loss:9009.2500, accuracy0.6900) \n",
      "           test (loss:12718.4941, accuracy0.5713)\n",
      "\n",
      "epoch:18    27.83sec \n",
      "           train(loss:8579.7871, accuracy0.7056) \n",
      "           test (loss:13003.1172, accuracy0.5704)\n",
      "\n",
      "epoch:19    27.32sec \n",
      "           train(loss:8260.6553, accuracy0.7135) \n",
      "           test (loss:12675.8105, accuracy0.5784)\n",
      "\n",
      "epoch:20    28.02sec \n",
      "           train(loss:8305.8418, accuracy0.7173) \n",
      "           test (loss:12609.3301, accuracy0.5689)\n",
      "\n",
      "epoch:21    27.25sec \n",
      "           train(loss:9205.8760, accuracy0.6833) \n",
      "           test (loss:13546.0000, accuracy0.5596)\n",
      "\n",
      "epoch:22    28.04sec \n",
      "           train(loss:8235.4951, accuracy0.7176) \n",
      "           test (loss:13132.3926, accuracy0.5625)\n",
      "\n",
      "epoch:23    27.33sec \n",
      "           train(loss:7511.2134, accuracy0.7439) \n",
      "           test (loss:12847.2725, accuracy0.5815)\n",
      "\n",
      "epoch:24    28.32sec \n",
      "           train(loss:7708.1860, accuracy0.7348) \n",
      "           test (loss:13077.3135, accuracy0.5719)\n",
      "\n",
      "epoch:25    27.43sec \n",
      "           train(loss:7295.3779, accuracy0.7538) \n",
      "           test (loss:13028.4287, accuracy0.5790)\n",
      "\n",
      "epoch:26    27.96sec \n",
      "           train(loss:7057.4697, accuracy0.7589) \n",
      "           test (loss:13425.1836, accuracy0.5761)\n",
      "\n",
      "epoch:27    27.59sec \n",
      "           train(loss:7425.8101, accuracy0.7459) \n",
      "           test (loss:14050.0234, accuracy0.5718)\n",
      "\n",
      "epoch:28    27.93sec \n",
      "           train(loss:7150.5903, accuracy0.7552) \n",
      "           test (loss:13973.1396, accuracy0.5723)\n",
      "\n",
      "epoch:29    27.44sec \n",
      "           train(loss:6737.7119, accuracy0.7715) \n",
      "           test (loss:13629.7373, accuracy0.5760)\n",
      "\n",
      "epoch:30    28.17sec \n",
      "           train(loss:6452.4502, accuracy0.7839) \n",
      "           test (loss:13978.9482, accuracy0.5760)\n",
      "\n",
      "epoch:31    27.19sec \n",
      "           train(loss:6516.8267, accuracy0.7790) \n",
      "           test (loss:14014.0186, accuracy0.5769)\n",
      "\n",
      "epoch:32    28.06sec \n",
      "           train(loss:6377.8169, accuracy0.7825) \n",
      "           test (loss:14201.0215, accuracy0.5711)\n",
      "\n",
      "epoch:33    27.24sec \n",
      "           train(loss:6117.7773, accuracy0.7957) \n",
      "           test (loss:14600.7002, accuracy0.5656)\n",
      "\n",
      "epoch:34    28.28sec \n",
      "           train(loss:6283.7661, accuracy0.7890) \n",
      "           test (loss:14745.9951, accuracy0.5709)\n",
      "\n",
      "epoch:35    27.48sec \n",
      "           train(loss:6360.6982, accuracy0.7826) \n",
      "           test (loss:14770.2305, accuracy0.5671)\n",
      "\n",
      "epoch:36    27.95sec \n",
      "           train(loss:6134.2109, accuracy0.7904) \n",
      "           test (loss:15061.3115, accuracy0.5658)\n",
      "\n",
      "epoch:37    27.34sec \n",
      "           train(loss:6284.4185, accuracy0.7877) \n",
      "           test (loss:15155.3457, accuracy0.5632)\n",
      "\n",
      "epoch:38    28.06sec \n",
      "           train(loss:6078.9355, accuracy0.7933) \n",
      "           test (loss:15134.4062, accuracy0.5599)\n",
      "\n",
      "epoch:39    27.43sec \n",
      "           train(loss:5597.5117, accuracy0.8149) \n",
      "           test (loss:15054.9600, accuracy0.5649)\n",
      "\n",
      "epoch:40    27.61sec \n",
      "           train(loss:5880.6221, accuracy0.8019) \n",
      "           test (loss:15079.0723, accuracy0.5633)\n",
      "\n",
      "epoch:41    27.40sec \n",
      "           train(loss:5753.0337, accuracy0.8087) \n",
      "           test (loss:15589.7646, accuracy0.5627)\n",
      "\n",
      "epoch:42    28.29sec \n",
      "           train(loss:5441.6504, accuracy0.8178) \n",
      "           test (loss:15769.4551, accuracy0.5649)\n",
      "\n",
      "epoch:43    27.51sec \n",
      "           train(loss:5164.6367, accuracy0.8316) \n",
      "           test (loss:15999.7168, accuracy0.5647)\n",
      "\n",
      "epoch:44    28.26sec \n",
      "           train(loss:5489.5474, accuracy0.8235) \n",
      "           test (loss:16581.6426, accuracy0.5453)\n",
      "\n",
      "epoch:45    27.58sec \n",
      "           train(loss:5579.6079, accuracy0.8089) \n",
      "           test (loss:16678.3555, accuracy0.5574)\n",
      "\n",
      "epoch:46    28.14sec \n",
      "           train(loss:5011.9258, accuracy0.8399) \n",
      "           test (loss:16127.4160, accuracy0.5624)\n",
      "\n",
      "epoch:47    27.47sec \n",
      "           train(loss:5323.1001, accuracy0.8222) \n",
      "           test (loss:16505.0430, accuracy0.5579)\n",
      "\n",
      "epoch:48    28.03sec \n",
      "           train(loss:4817.6958, accuracy0.8477) \n",
      "           test (loss:16758.6133, accuracy0.5613)\n",
      "\n",
      "epoch:49    27.52sec \n",
      "           train(loss:5070.6489, accuracy0.8343) \n",
      "           test (loss:16489.1895, accuracy0.5544)\n",
      "\n",
      "epoch:50    28.40sec \n",
      "           train(loss:5790.1904, accuracy0.8086) \n",
      "           test (loss:17552.4375, accuracy0.5394)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_path_train = []\n",
    "loss_path_test = []\n",
    "\n",
    "embedding_train = []\n",
    "embedding_test = []\n",
    "\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "iter_cnt = 50\n",
    "np.random.seed(params['seed'])\n",
    "sample_size = 10000 # use 1000 samples only for monitoring\n",
    "\n",
    "# initial embedding train/test \n",
    "embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]})) \n",
    "\n",
    "# initialize centroids\n",
    "centroids_list = []\n",
    "train_idx_list = np.arange(len(y_train))\n",
    "chunk_idx_by_label = {x:train_idx_list[y_train == x] for x in params['labels']} \n",
    "for label in params['labels']:\n",
    "    class_sample_embedding = sess.run(embedding_layer, feed_dict={x:x_train[chunk_idx_by_label[label]]})\n",
    "    centroids_list.append(np.mean(class_sample_embedding, axis=0))\n",
    "centroids_list = np.array(centroids_list)\n",
    "\n",
    "for epoch in range(iter_cnt):\n",
    "    start_time = time.time()\n",
    "    cursor = 0\n",
    "    step = 1\n",
    "    \n",
    "    # random shuffle\n",
    "    train_idx = np.arange(len(y_train))\n",
    "    np.random.shuffle(train_idx)\n",
    "    shuffled_x_train = x_train[train_idx]\n",
    "    shuffled_y_train = y_train[train_idx]\n",
    "    shuffled_y_train_ohe = y_train_ohe[train_idx]\n",
    "\n",
    "    while cursor < len(y_train): \n",
    "        batch_x_train = shuffled_x_train[cursor:cursor+params['batch_size']]\n",
    "        batch_y_train = shuffled_y_train[cursor:cursor+params['batch_size']]\n",
    "        batch_y_train_ohe = shuffled_y_train_ohe[cursor:cursor+params['batch_size']] \n",
    "        sess.run(train_model, feed_dict={x:batch_x_train,\n",
    "                                         y_ohe:batch_y_train_ohe,\n",
    "                                         y:batch_y_train,\n",
    "                                         centroids:centroids_list}) \n",
    "        \n",
    "        train_idx_list = np.arange(len(batch_y_train))\n",
    "        chunk_idx_by_label = {x:train_idx_list[batch_y_train == x] for x in params['labels']} \n",
    "        # update centroids\n",
    "        for label in params['labels']:\n",
    "            if len(batch_x_train[chunk_idx_by_label[label]]) != 0:\n",
    "                class_sample_embedding = sess.run(embedding_layer, feed_dict={x:batch_x_train[chunk_idx_by_label[label]]})\n",
    "                delta = np.sum(centroids_list[label] - class_sample_embedding)\n",
    "                delta = delta / (1+len(class_sample_embedding))\n",
    "                centroids_list[label] -= params['alpha'] * delta \n",
    "            \n",
    "        step += 1\n",
    "        cursor += params['batch_size']\n",
    "    \n",
    "    # embedding train/test\n",
    "    embedding_train.append(sess.run(embedding_layer, feed_dict={x:x_train[:sample_size]}))\n",
    "    embedding_test.append(sess.run(embedding_layer, feed_dict={x:x_test[:sample_size]}))\n",
    "    \n",
    "    # loss train/test\n",
    "    loss_path_train.append(sess.run(loss, feed_dict={x:x_train[:sample_size], y:y_train[:sample_size], y_ohe:y_train_ohe[:sample_size], centroids:centroids_list}))\n",
    "    loss_path_test.append(sess.run(loss, feed_dict={x:x_test[:sample_size], y:y_test[:sample_size], y_ohe:y_test_ohe[:sample_size], centroids:centroids_list}))\n",
    "    \n",
    "    # loss train/test\n",
    "    loss_path_train.append(sess.run(loss, feed_dict={x:x_train[:sample_size], y_ohe:y_train_ohe[:sample_size], y:y_train[:sample_size], centroids:centroids_list}))\n",
    "    loss_path_test.append(sess.run(loss, feed_dict={x:x_test[:sample_size], y_ohe:y_test_ohe[:sample_size], y:y_test[:sample_size], centroids:centroids_list}))\n",
    "    \n",
    "    # accuracy train/test\n",
    "    pred_train = pd.DataFrame(sess.run(softmax_layer, feed_dict={x:x_train[:sample_size]})).idxmax(axis=1)\n",
    "    accuracy_train.append(np.sum((pred_train == y_train[:sample_size]).astype('int')) / len(y_train[:sample_size]))\n",
    "    pred_test = pd.DataFrame(sess.run(softmax_layer, feed_dict={x:x_test[:sample_size]})).idxmax(axis=1)\n",
    "    accuracy_test.append(np.sum((pred_test == y_test[:sample_size]).astype('int')) / len(y_test[:sample_size]))\n",
    "        \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('epoch:{}    {:.2f}sec \\n\\\n",
    "           train(loss:{:.4f}, accuracy{:.4f}) \\n\\\n",
    "           test (loss:{:.4f}, accuracy{:.4f})'.format(\n",
    "           epoch+1, end_time - start_time,\n",
    "           loss_path_train[-1], accuracy_train[-1],\n",
    "           loss_path_test[-1], accuracy_test[-1]))\n",
    "    print('')\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
